{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Conv2D, Reshape, MaxPooling2D, UpSampling2D, Flatten, BatchNormalization, Dropout\n",
    "from keras.models import Model, load_model\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "class FeatureExtractor:\n",
    "    def __init__(self):\n",
    "        self.encoder_model = None\n",
    "        self.model = None\n",
    "        return\n",
    "    \n",
    "    def build(self, input_dims, opt):\n",
    "        input_layer = Input(shape=input_dims)\n",
    "        \n",
    "        a_one = Conv2D(64, (3,3), activation='relu', padding='same') (input_layer)\n",
    "        #a_two = BatchNormalization() (a_one)\n",
    "        a_three = Conv2D(64, (3,3), activation='relu', padding='same') (a_one)\n",
    "        #a_four = BatchNormalization() (a_three)\n",
    "        a_five = MaxPooling2D() (a_three)\n",
    "        block_one = Dropout(0.25) (a_five)\n",
    "        #block_one = a_five\n",
    "        \n",
    "        b_one = Conv2D(128, (3,3), activation='relu', padding='same') (block_one)\n",
    "        #b_two = BatchNormalization() (b_one)\n",
    "        b_three = Conv2D(128, (3,3), activation='relu', padding='same') (b_one)\n",
    "        #b_four = BatchNormalization() (b_two)\n",
    "        b_five = MaxPooling2D() (b_three)\n",
    "        block_two = Dropout(0.25) (b_five)\n",
    "        \n",
    "        c_one = Conv2D(256, (3,3), activation='relu', padding='same') (block_two)\n",
    "        #c_two = BatchNormalization() (c_one)\n",
    "        c_three = Conv2D(256, (3,3), activation='relu', padding='same') (c_one)\n",
    "        #c_four = BatchNormalization() (c_three)\n",
    "        c_five = MaxPooling2D() (c_three)\n",
    "        block_three = Dropout(0.5) (c_five)\n",
    "        \n",
    "        d_one = Conv2D(512, (3,3), activation='relu', padding='same') (block_three)\n",
    "        #d_two = BatchNormalization() (d_one)\n",
    "        d_three = Conv2D(512, (1,1), activation='relu', padding='same') (d_one)\n",
    "        #d_four = BatchNormalization() (d_three)\n",
    "        d_five = MaxPooling2D() (d_three)\n",
    "        block_four = Dropout(0.2) (d_five)\n",
    "        \n",
    "        flat = Flatten() (block_four)\n",
    "        fc_one = Dense(4096, activation='relu') (flat)\n",
    "        #block_five = BatchNormalization() (fc_one)\n",
    "        \n",
    "        fc_two = Dense(4096, activation='relu') (fc_one)\n",
    "        #block_six = BatchNormalization() (fc_two)\n",
    "        \n",
    "        final = Dense(4, activation='softmax') (fc_two)\n",
    "        \n",
    "        self.model = Model(input_layer, final)\n",
    "        self.feature_extractor = Model(input_layer, flat)\n",
    "        self.model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        return\n",
    "    \n",
    "    def load(self, model_file, encoder_model_file):\n",
    "        self.encoder_model = load_model(encoder_model_file)\n",
    "        self.model = load_model(model_file)\n",
    "        return\n",
    "    \n",
    "    def train(self, train_input, train_output,\n",
    "             val_input, val_output,\n",
    "             epochs=50,\n",
    "             batch_size=64,\n",
    "             shuffle=True):\n",
    "        tensorboard = TensorBoard(log_dir='./tf_logs', histogram_freq=0, write_graph=True, write_images=False)\n",
    "        self.model.fit(train_input, train_output,\n",
    "                      epochs=epochs, batch_size=batch_size,\n",
    "                      shuffle=shuffle,\n",
    "                      validation_data=(val_input, val_output),\n",
    "                      callbacks=[tensorboard])\n",
    "        return\n",
    "    \n",
    "    def encoder_predict(self, test_input):\n",
    "        return self.encoder_model.predict(test_input)\n",
    "    \n",
    "    def predict(self, test_input):\n",
    "        return self.model.predict(test_input)\n",
    "    \n",
    "    def save(self, model_file, encoder_model_file):\n",
    "        self.model.save(model_file)\n",
    "        self.encoder_model.save(encoder_model_file)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 32, 32, 3)\n",
      "(4000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import cifar10\n",
    "from keras import optimizers\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "interested = [0, 1, 8, 9]\n",
    "\n",
    "scrap = []\n",
    "for idx, im in enumerate(x_train):\n",
    "    if (y_train[idx][0] not in interested):\n",
    "        scrap.append(idx)\n",
    "        \n",
    "x_train = np.delete(x_train, scrap, axis=0)\n",
    "y_train = np.delete(y_train, scrap, axis=0)\n",
    "\n",
    "enc = preprocessing.OneHotEncoder()\n",
    "enc.fit(y_train)\n",
    "y_train = enc.transform(y_train).toarray()\n",
    "\n",
    "scrap = []\n",
    "for idx, im in enumerate(x_test):\n",
    "    if (y_test[idx][0] not in interested):\n",
    "        scrap.append(idx)\n",
    "x_test = np.delete(x_test, scrap, axis=0)\n",
    "y_test = np.delete(y_test, scrap, axis=0)\n",
    "y_test = enc.transform(y_test).toarray()\n",
    "\n",
    "x_train = (x_train.astype('float32')) / 255.0\n",
    "x_test = (x_test.astype('float32')) / 255.0\n",
    "\n",
    "print x_train.shape\n",
    "print x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fe = FeatureExtractor()\n",
    "opt = optimizers.adam(lr=0.0001, decay=1e-6)\n",
    "#opt = optimizers.rmsprop()\n",
    "#opt = optimizers.SGD(lr=0.1, nesterov=True, momentum=0.9)\n",
    "fe.build((32, 32, 3, ), opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 4000 samples\n",
      "Epoch 1/100\n",
      "20000/20000 [==============================] - 80s - loss: 1.2374 - acc: 0.3981 - val_loss: 1.0555 - val_acc: 0.5413\n",
      "Epoch 2/100\n",
      "20000/20000 [==============================] - 77s - loss: 1.0317 - acc: 0.5354 - val_loss: 0.9526 - val_acc: 0.6102\n",
      "Epoch 3/100\n",
      "20000/20000 [==============================] - 77s - loss: 0.9014 - acc: 0.6035 - val_loss: 0.7927 - val_acc: 0.6783\n",
      "Epoch 4/100\n",
      "20000/20000 [==============================] - 77s - loss: 0.7704 - acc: 0.6835 - val_loss: 0.7420 - val_acc: 0.6917\n",
      "Epoch 5/100\n",
      "20000/20000 [==============================] - 77s - loss: 0.6485 - acc: 0.7446 - val_loss: 0.5632 - val_acc: 0.7865\n",
      "Epoch 6/100\n",
      "20000/20000 [==============================] - 77s - loss: 0.5620 - acc: 0.7847 - val_loss: 0.4957 - val_acc: 0.8133\n",
      "Epoch 7/100\n",
      "20000/20000 [==============================] - 77s - loss: 0.4844 - acc: 0.8179 - val_loss: 0.4462 - val_acc: 0.8307\n",
      "Epoch 8/100\n",
      "20000/20000 [==============================] - 77s - loss: 0.4444 - acc: 0.8350 - val_loss: 0.3907 - val_acc: 0.8555\n",
      "Epoch 9/100\n",
      "20000/20000 [==============================] - 77s - loss: 0.4024 - acc: 0.8539 - val_loss: 0.3669 - val_acc: 0.8658\n",
      "Epoch 10/100\n",
      "20000/20000 [==============================] - 77s - loss: 0.3676 - acc: 0.8626 - val_loss: 0.3683 - val_acc: 0.8645\n",
      "Epoch 11/100\n",
      "20000/20000 [==============================] - 77s - loss: 0.3400 - acc: 0.8750 - val_loss: 0.3770 - val_acc: 0.8718\n",
      "Epoch 12/100\n",
      "20000/20000 [==============================] - 77s - loss: 0.3203 - acc: 0.8819 - val_loss: 0.3421 - val_acc: 0.8782\n",
      "Epoch 13/100\n",
      "20000/20000 [==============================] - 77s - loss: 0.2928 - acc: 0.8941 - val_loss: 0.3211 - val_acc: 0.8845\n",
      "Epoch 14/100\n",
      "20000/20000 [==============================] - 80s - loss: 0.2807 - acc: 0.8975 - val_loss: 0.2992 - val_acc: 0.8918\n",
      "Epoch 15/100\n",
      "20000/20000 [==============================] - 83s - loss: 0.2630 - acc: 0.9033 - val_loss: 0.3324 - val_acc: 0.8825\n",
      "Epoch 16/100\n",
      "20000/20000 [==============================] - 77s - loss: 0.2449 - acc: 0.9102 - val_loss: 0.3227 - val_acc: 0.8828\n",
      "Epoch 17/100\n",
      "20000/20000 [==============================] - 178s - loss: 0.2308 - acc: 0.9164 - val_loss: 0.2932 - val_acc: 0.9002\n",
      "Epoch 18/100\n",
      "20000/20000 [==============================] - 238s - loss: 0.2194 - acc: 0.9202 - val_loss: 0.2857 - val_acc: 0.8985\n",
      "Epoch 19/100\n",
      "20000/20000 [==============================] - 273s - loss: 0.2054 - acc: 0.9253 - val_loss: 0.2802 - val_acc: 0.8998\n",
      "Epoch 20/100\n",
      "20000/20000 [==============================] - 136s - loss: 0.1858 - acc: 0.9308 - val_loss: 0.2735 - val_acc: 0.9035\n",
      "Epoch 21/100\n",
      "20000/20000 [==============================] - 106s - loss: 0.1833 - acc: 0.9325 - val_loss: 0.2723 - val_acc: 0.9077\n",
      "Epoch 22/100\n",
      "20000/20000 [==============================] - 106s - loss: 0.1683 - acc: 0.9396 - val_loss: 0.2911 - val_acc: 0.9032\n",
      "Epoch 23/100\n",
      "20000/20000 [==============================] - 123s - loss: 0.1585 - acc: 0.9410 - val_loss: 0.3253 - val_acc: 0.9032\n",
      "Epoch 24/100\n",
      "20000/20000 [==============================] - 120s - loss: 0.1467 - acc: 0.9460 - val_loss: 0.2974 - val_acc: 0.9062\n",
      "Epoch 25/100\n",
      "20000/20000 [==============================] - 149s - loss: 0.1410 - acc: 0.9485 - val_loss: 0.2952 - val_acc: 0.9095\n",
      "Epoch 26/100\n",
      "20000/20000 [==============================] - 94s - loss: 0.1314 - acc: 0.9528 - val_loss: 0.2839 - val_acc: 0.9075\n",
      "Epoch 27/100\n",
      "20000/20000 [==============================] - 133s - loss: 0.1286 - acc: 0.9537 - val_loss: 0.3148 - val_acc: 0.9065\n",
      "Epoch 28/100\n",
      "20000/20000 [==============================] - 178s - loss: 0.1170 - acc: 0.9578 - val_loss: 0.2919 - val_acc: 0.9113\n",
      "Epoch 29/100\n",
      "20000/20000 [==============================] - 138s - loss: 0.1114 - acc: 0.9601 - val_loss: 0.3137 - val_acc: 0.9093\n",
      "Epoch 30/100\n",
      "20000/20000 [==============================] - 142s - loss: 0.1087 - acc: 0.9591 - val_loss: 0.2665 - val_acc: 0.9185\n",
      "Epoch 31/100\n",
      "20000/20000 [==============================] - 87s - loss: 0.1030 - acc: 0.9621 - val_loss: 0.2716 - val_acc: 0.9163\n",
      "Epoch 32/100\n",
      "20000/20000 [==============================] - 91s - loss: 0.0900 - acc: 0.9667 - val_loss: 0.3211 - val_acc: 0.9125\n",
      "Epoch 33/100\n",
      "20000/20000 [==============================] - 165s - loss: 0.1004 - acc: 0.9637 - val_loss: 0.2950 - val_acc: 0.9160\n",
      "Epoch 34/100\n",
      "20000/20000 [==============================] - 79s - loss: 0.0898 - acc: 0.9673 - val_loss: 0.3070 - val_acc: 0.9133\n",
      "Epoch 35/100\n",
      "20000/20000 [==============================] - 97s - loss: 0.0789 - acc: 0.9708 - val_loss: 0.3140 - val_acc: 0.9150\n",
      "Epoch 36/100\n",
      "20000/20000 [==============================] - 107s - loss: 0.0816 - acc: 0.9701 - val_loss: 0.3400 - val_acc: 0.9105\n",
      "Epoch 37/100\n",
      "20000/20000 [==============================] - 173s - loss: 0.0806 - acc: 0.9708 - val_loss: 0.3141 - val_acc: 0.9147\n",
      "Epoch 38/100\n",
      "20000/20000 [==============================] - 144s - loss: 0.0706 - acc: 0.9741 - val_loss: 0.2984 - val_acc: 0.9197\n",
      "Epoch 39/100\n",
      "20000/20000 [==============================] - 189s - loss: 0.0626 - acc: 0.9780 - val_loss: 0.3094 - val_acc: 0.9185\n",
      "Epoch 40/100\n",
      "20000/20000 [==============================] - 256s - loss: 0.0641 - acc: 0.9764 - val_loss: 0.3484 - val_acc: 0.9093\n",
      "Epoch 41/100\n",
      "20000/20000 [==============================] - 187s - loss: 0.0668 - acc: 0.9764 - val_loss: 0.3045 - val_acc: 0.9200\n",
      "Epoch 42/100\n",
      "20000/20000 [==============================] - 190s - loss: 0.0617 - acc: 0.9773 - val_loss: 0.3044 - val_acc: 0.9200\n",
      "Epoch 43/100\n",
      "20000/20000 [==============================] - 202s - loss: 0.0566 - acc: 0.9791 - val_loss: 0.3239 - val_acc: 0.9217\n",
      "Epoch 44/100\n",
      "20000/20000 [==============================] - 121s - loss: 0.0573 - acc: 0.9798 - val_loss: 0.3208 - val_acc: 0.9207\n",
      "Epoch 45/100\n",
      "20000/20000 [==============================] - 127s - loss: 0.0528 - acc: 0.9799 - val_loss: 0.3078 - val_acc: 0.9223\n",
      "Epoch 46/100\n",
      "20000/20000 [==============================] - 105s - loss: 0.0490 - acc: 0.9819 - val_loss: 0.4052 - val_acc: 0.9177\n",
      "Epoch 47/100\n",
      "20000/20000 [==============================] - 193s - loss: 0.0534 - acc: 0.9797 - val_loss: 0.3381 - val_acc: 0.9167\n",
      "Epoch 48/100\n",
      "20000/20000 [==============================] - 202s - loss: 0.0503 - acc: 0.9825 - val_loss: 0.3651 - val_acc: 0.9153\n",
      "Epoch 49/100\n",
      "20000/20000 [==============================] - 163s - loss: 0.0475 - acc: 0.9842 - val_loss: 0.3175 - val_acc: 0.9235\n",
      "Epoch 50/100\n",
      "20000/20000 [==============================] - 97s - loss: 0.0416 - acc: 0.9846 - val_loss: 0.3614 - val_acc: 0.9167\n",
      "Epoch 51/100\n",
      "20000/20000 [==============================] - 114s - loss: 0.0430 - acc: 0.9850 - val_loss: 0.3379 - val_acc: 0.9257\n",
      "Epoch 52/100\n",
      "20000/20000 [==============================] - 136s - loss: 0.0409 - acc: 0.9845 - val_loss: 0.3548 - val_acc: 0.9185\n",
      "Epoch 53/100\n",
      "20000/20000 [==============================] - 99s - loss: 0.0360 - acc: 0.9876 - val_loss: 0.3890 - val_acc: 0.9135\n",
      "Epoch 54/100\n",
      "20000/20000 [==============================] - 104s - loss: 0.0482 - acc: 0.9816 - val_loss: 0.3182 - val_acc: 0.9173\n",
      "Epoch 55/100\n",
      "20000/20000 [==============================] - 181s - loss: 0.0413 - acc: 0.9850 - val_loss: 0.3424 - val_acc: 0.9237\n",
      "Epoch 56/100\n",
      "20000/20000 [==============================] - 200s - loss: 0.0405 - acc: 0.9851 - val_loss: 0.3895 - val_acc: 0.9175\n",
      "Epoch 57/100\n",
      "20000/20000 [==============================] - 138s - loss: 0.0390 - acc: 0.9863 - val_loss: 0.3165 - val_acc: 0.9223\n",
      "Epoch 58/100\n",
      "20000/20000 [==============================] - 138s - loss: 0.0350 - acc: 0.9873 - val_loss: 0.3359 - val_acc: 0.9263\n",
      "Epoch 59/100\n",
      "20000/20000 [==============================] - 171s - loss: 0.0307 - acc: 0.9891 - val_loss: 0.3619 - val_acc: 0.9217\n",
      "Epoch 60/100\n",
      "20000/20000 [==============================] - 181s - loss: 0.0399 - acc: 0.9867 - val_loss: 0.3609 - val_acc: 0.9230\n",
      "Epoch 61/100\n",
      "20000/20000 [==============================] - 122s - loss: 0.0340 - acc: 0.9874 - val_loss: 0.3844 - val_acc: 0.9153\n",
      "Epoch 62/100\n",
      "20000/20000 [==============================] - 192s - loss: 0.0384 - acc: 0.9862 - val_loss: 0.3597 - val_acc: 0.9207\n",
      "Epoch 63/100\n",
      "20000/20000 [==============================] - 151s - loss: 0.0285 - acc: 0.9898 - val_loss: 0.3327 - val_acc: 0.9250\n",
      "Epoch 64/100\n",
      "20000/20000 [==============================] - 78s - loss: 0.0299 - acc: 0.9898 - val_loss: 0.3690 - val_acc: 0.9215\n",
      "Epoch 65/100\n",
      " 4224/20000 [=====>........................] - ETA: 60s - loss: 0.0288 - acc: 0.9901"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fe.train(x_train, y_train, x_test, y_test,\n",
    "                 epochs=100,\n",
    "                 batch_size=128,\n",
    "                 shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved!\n"
     ]
    }
   ],
   "source": [
    "fe.save('extractor.h5', 'extractor-model.h5')\n",
    "print \"Model saved!\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
